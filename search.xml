<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>线性回归算法</title>
    <url>/2022/12/13/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="1-线性回归算法简介"><a href="#1-线性回归算法简介" class="headerlink" title="1.线性回归算法简介"></a>1.线性回归算法简介</h1><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-28f661e8edd149d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="1"
                ></p>
<p>线性回归算法以一个坐标系里一个维度为结果，其他维度为特征（如二维平面坐标系中横轴为特征，纵轴为结果），无数的训练集放在坐标系中，发现他们是围绕着一条执行分布。线性回归算法的期望，就是寻找一条直线，最大程度的“拟合”样本特征和样本输出标记的关系<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-ed83e4dab6237239.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="2"
                ></p>
<h5 id="样本特征只有一个的线性回归问题，为简单线性回归，如房屋价格-房屋面积"><a href="#样本特征只有一个的线性回归问题，为简单线性回归，如房屋价格-房屋面积" class="headerlink" title="样本特征只有一个的线性回归问题，为简单线性回归，如房屋价格-房屋面积"></a>样本特征只有一个的线性回归问题，为简单线性回归，如房屋价格-房屋面积</h5><p>将横坐标作为x轴，纵坐标作为y轴，每一个点为（X(i) ,y(i)）,那么我们期望寻找的直线就是y&#x3D;ax+b，当给出一个新的点x(j)的时候，我们希望预测的y^(j)&#x3D;ax(j)+b</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-9907accb4deda2e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="7"
                ></p>
<ul>
<li>不使用直接相减的方式，由于差值有正有负，会抵消</li>
<li>不适用绝对值的方式，由于绝对值函数存在不可导的点<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-674c2856a8eb6ccb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="8"
                ></li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-a5b3ed8589e06778.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="11"
                ></p>
<h5 id="通过上面的推导，我们可以归纳出一类机器学习算法的基本思路，如下图；其中损失函数是计算期望值和预测值的差值，期望其差值（也就是损失）越来越小，而效用函数则是描述拟合度，期望契合度越来越好"><a href="#通过上面的推导，我们可以归纳出一类机器学习算法的基本思路，如下图；其中损失函数是计算期望值和预测值的差值，期望其差值（也就是损失）越来越小，而效用函数则是描述拟合度，期望契合度越来越好" class="headerlink" title="通过上面的推导，我们可以归纳出一类机器学习算法的基本思路，如下图；其中损失函数是计算期望值和预测值的差值，期望其差值（也就是损失）越来越小，而效用函数则是描述拟合度，期望契合度越来越好"></a>通过上面的推导，我们可以归纳出一类机器学习算法的基本思路，如下图；其中损失函数是计算期望值和预测值的差值，期望其差值（也就是损失）越来越小，而效用函数则是描述拟合度，期望契合度越来越好</h5><h2 id=""><a href="#" class="headerlink" title=""></a><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-4d9f03a1442afd3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="9"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-13cf3c2d50fbb1f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="10"
                ></h2><h1 id="2-简单线性回归的实现"><a href="#2-简单线性回归的实现" class="headerlink" title="2.简单线性回归的实现"></a>2.简单线性回归的实现</h1><h2 id="2-1-for循环方式实现"><a href="#2-1-for循环方式实现" class="headerlink" title="2.1 for循环方式实现"></a>2.1 for循环方式实现</h2><ul>
<li><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2></li>
</ul>
<h3 id="a-b公式"><a href="#a-b公式" class="headerlink" title="a,b公式"></a>a,b公式</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-d6a7d9184027fd0e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="2.2-2"
                ></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">class SimpleLinearRegression1:</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        &quot;&quot;&quot;初始化Simple Linear Regression 模型&quot;&quot;&quot;</span><br><span class="line">        self.a_ = None</span><br><span class="line">        self.b_ = None</span><br><span class="line"></span><br><span class="line">    def fit(self, x_train, y_train):</span><br><span class="line">        &quot;&quot;&quot;根据训练集x_train，y_train 训练Simple Linear Regression 模型&quot;&quot;&quot;</span><br><span class="line">        assert x_train.ndim == 1,\</span><br><span class="line">            &quot;Simple Linear Regression can only solve simple feature training data&quot;</span><br><span class="line">        assert len(x_train) == len(y_train),\</span><br><span class="line">            &quot;the size of x_train must be equal to the size of y_train&quot;</span><br><span class="line"></span><br><span class="line">        # 求均值</span><br><span class="line">        x_mean = x_train.mean()</span><br><span class="line">        y_mean = y_train.mean()</span><br><span class="line"></span><br><span class="line">        # 分子</span><br><span class="line">        num = 0.0</span><br><span class="line">        # 分母</span><br><span class="line">        d = 0.0</span><br><span class="line"></span><br><span class="line">        # 计算分子分母</span><br><span class="line">        for x_i, y_i in zip(x_train, y_train):</span><br><span class="line">            num += (x_i-x_mean)*(y_i-y_mean)</span><br><span class="line">            d += (x_i-x_mean) ** 2</span><br><span class="line"></span><br><span class="line">        # 计算参数a和b</span><br><span class="line">        self.a_ = num/d</span><br><span class="line">        self.b_ = y_mean - self.a_ * x_mean</span><br><span class="line"></span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def predict(self, x_predict):</span><br><span class="line">        &quot;&quot;&quot;给定待预测集x_predict，返回x_predict对应的预测结果值&quot;&quot;&quot;</span><br><span class="line">        assert x_predict.ndim == 1,\</span><br><span class="line">            &quot;Simple Linear Regression can only solve simple feature training data&quot;</span><br><span class="line">        assert self.a_ is not None and self.b_ is not None,\</span><br><span class="line">            &quot;must fit before predict!&quot;</span><br><span class="line"></span><br><span class="line">        return np.array([self._predict(x) for x in x_predict])</span><br><span class="line"></span><br><span class="line">    def _predict(self, x_single):</span><br><span class="line">        &quot;&quot;&quot;给定单个待预测数据x_single，返回x_single对应的预测结果值&quot;&quot;&quot;</span><br><span class="line">        return self.a_*x_single+self.b_</span><br><span class="line"></span><br><span class="line">    def __repr__(self):</span><br><span class="line">        return &quot;SimpleLinearRegression1()&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
<ul>
<li><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2></li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure></div>

<h3 id="简单自定义一个训练集并描绘"><a href="#简单自定义一个训练集并描绘" class="headerlink" title="简单自定义一个训练集并描绘"></a>简单自定义一个训练集并描绘</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array([<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>,<span class="number">4.</span>,<span class="number">5.</span>])</span><br><span class="line">y = np.array([<span class="number">1.</span>,<span class="number">3.</span>,<span class="number">2.</span>,<span class="number">3.</span>,<span class="number">5.</span>])</span><br><span class="line">plt.scatter(x,y)</span><br><span class="line">plt.axis([<span class="number">0</span>,<span class="number">6</span>,<span class="number">0</span>,<span class="number">6</span>])</span><br></pre></td></tr></table></figure></div>




<pre><code>[0, 6, 0, 6]
</code></pre>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-ef43bf85ec0b3298.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="2.1-1"
                ></p>
<h3 id="使用我们自己的SimpleLinearRegression1"><a href="#使用我们自己的SimpleLinearRegression1" class="headerlink" title="使用我们自己的SimpleLinearRegression1"></a>使用我们自己的SimpleLinearRegression1</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> machine_learning.SimpleLinearRegression1 <span class="keyword">import</span> SimpleLinearRegression1</span><br><span class="line"></span><br><span class="line">reg1 = SimpleLinearRegression1()</span><br><span class="line">reg1.fit(x,y)</span><br><span class="line"><span class="comment"># 输出  SimpleLinearRegression1()</span></span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">y_predict = reg1.predict(np.array([<span class="number">6.</span>]))</span><br><span class="line">y_predict</span><br><span class="line"><span class="comment">#   输出  array([5.2])</span></span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">reg1.a_</span><br><span class="line"> <span class="comment">#  0.8</span></span><br><span class="line">reg1.b_</span><br><span class="line"><span class="comment">#     0.39999999999999947</span></span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">y_hat = reg1.predict(x)</span><br><span class="line">plt.scatter(x,y)</span><br><span class="line">plt.plot(x,y_hat,color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>,<span class="number">6</span>,<span class="number">0</span>,<span class="number">6</span>])</span><br></pre></td></tr></table></figure></div>

<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-2f448da2a1a38cf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="2.2-2"
                ></p>
<h2 id="2-2-向量化"><a href="#2-2-向量化" class="headerlink" title="2.2 向量化"></a>2.2 向量化</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-6abe87c3160fd366.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="2.2-1"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-215060bbf7fc403b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="2.2-2"
                ></p>
<h3 id="向量化改进num-d的计算方法"><a href="#向量化改进num-d的计算方法" class="headerlink" title="向量化改进num,d的计算方法"></a>向量化改进num,d的计算方法</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用向量化点乘计算分子和分母</span><br><span class="line">num = (x_train-x_mean).dot(y_train-y_mean)</span><br><span class="line">d = (x_train-x_mean).dot(x_train-x_mean)</span><br></pre></td></tr></table></figure></div>
<h3 id="向量化实现的性能测试"><a href="#向量化实现的性能测试" class="headerlink" title="向量化实现的性能测试"></a>向量化实现的性能测试</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">m = <span class="number">1000000</span></span><br><span class="line">big_x = np.random.random(size=m)</span><br><span class="line">big_y = big_x * <span class="number">2.0</span> + <span class="number">3.0</span> + np.random.normal(size=m)</span><br></pre></td></tr></table></figure></div>


<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">%timeit reg1.fit(big_x,big_y)</span><br><span class="line">%timeit reg2.fit(big_x,big_y)</span><br></pre></td></tr></table></figure></div>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 输出</span><br><span class="line">826 ms ± 6.93 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br><span class="line">11.3 ms ± 84.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</span><br></pre></td></tr></table></figure></div>
<p>可以看出，向量化的运行速度比循环的形式速度要快80倍</p>
<h1 id="3-衡量线性回归算法的指标"><a href="#3-衡量线性回归算法的指标" class="headerlink" title="3.衡量线性回归算法的指标"></a>3.衡量线性回归算法的指标</h1><h2 id="3-1-衡量标准"><a href="#3-1-衡量标准" class="headerlink" title="3.1 衡量标准"></a>3.1 衡量标准</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-c440c129cd28b499.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="3.1"
                ><br>其中衡量标准是和m有关的，因为越多的数据量产生的误差和可能会更大，但是毫无疑问越多的数据量训练出来的模型更好，为此需要一个取消误差的方法，如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-31090a2f956341f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="3.2"
                ><br>MSE 的缺点，量纲不准确，如果y的单位是万元，平方后就变成了万元的平方，这可能会给我们带来一些麻烦<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-250c94798e4c458c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="3.3"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-ace4bb2857d3417f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="3.4"
                ><br>RMSE 平方累加后再开根号，如果某些预测结果和真实结果相差非常大，那么RMSE的结果会相对变大，所以RMSE有放大误差的趋势，而MAE没有，他直接就反应的是预测结果和真实结果直接的差距，正因如此，从某种程度上来说，想办法我们让RMSE变的更小小对于我们来说比较有意义，因为这意味着整个样本的错误中，那个最值相对比较小，而且我们之前训练样本的目标，就是RMSE根号里面1&#x2F;m的这一部分，而这一部分的本质和优化RMSE是一样的</p>
<h2 id="3-2-MSE-RMSE-MAE的实现"><a href="#3-2-MSE-RMSE-MAE的实现" class="headerlink" title="3.2 MSE,RMSE,MAE的实现"></a>3.2 MSE,RMSE,MAE的实现</h2><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">def mean_squared_error(y_true, y_predict):</span><br><span class="line">    &quot;&quot;&quot;计算y_true和y_predict之间的MSE&quot;&quot;&quot;</span><br><span class="line">    assert len(y_true) == len(y_predict), \</span><br><span class="line">        &quot;the size of y_true must be equal to the size of y_predict&quot;</span><br><span class="line"></span><br><span class="line">    return np.sum((y_true - y_predict)**2) / len(y_true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def root_mean_squared_error(y_true, y_predict):</span><br><span class="line">    &quot;&quot;&quot;计算y_true和y_predict之间的RMSE&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    return sqrt(mean_squared_error(y_true, y_predict))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def mean_absolute_error(y_true, y_predict):</span><br><span class="line">    &quot;&quot;&quot;计算y_true和y_predict之间的RMSE&quot;&quot;&quot;</span><br><span class="line">    assert len(y_true) == len(y_predict), \</span><br><span class="line">        &quot;the size of y_true must be equal to the size of y_predict&quot;</span><br><span class="line"></span><br><span class="line">    return np.sum(np.absolute(y_true - y_predict)) / len(y_true)</span><br></pre></td></tr></table></figure></div>
<h2 id="3-3-调用sikit-learn-的实现"><a href="#3-3-调用sikit-learn-的实现" class="headerlink" title="3.3 调用sikit learn 的实现"></a>3.3 调用sikit learn 的实现</h2><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.metrics import mean_squared_error</span><br><span class="line">from sklearn.metrics import mean_absolute_error</span><br><span class="line">mean_squared_error(y_test,y_predict)</span><br></pre></td></tr></table></figure></div>



<h1 id="4-最好的衡量线性回归法的指标-R-Squared"><a href="#4-最好的衡量线性回归法的指标-R-Squared" class="headerlink" title="4.最好的衡量线性回归法的指标 R Squared"></a>4.最好的衡量线性回归法的指标 R Squared</h1><h3 id="RMSE-和-MAE的局限性"><a href="#RMSE-和-MAE的局限性" class="headerlink" title="RMSE 和 MAE的局限性"></a>RMSE 和 MAE的局限性</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-879bb4bcbb132790.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="4.1"
                ><br>可能预测房源准确度，RMSE或者MAE的值为5，预测学生的分数，结果的误差是10，这个5和10没有判断性，因为5和10对应不同的单位和量纲，无法比较</p>
<h2 id="4-1-解决办法-R-Squared简介"><a href="#4-1-解决办法-R-Squared简介" class="headerlink" title="4.1 解决办法-R Squared简介"></a>4.1 解决办法-R Squared简介</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-9c44823246ca9510.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="4.1-1"
                ></p>
<h3 id="4-1-1-R-Squared-意义"><a href="#4-1-1-R-Squared-意义" class="headerlink" title="4.1.1 R Squared 意义"></a>4.1.1 R Squared 意义</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-c45d712c01b512f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="4.1-2"
                ><br>使用BaseLine Model产生的错误会很大，使用我们的模型预测产生的错误会相对少些（因为我们的模型充分的考虑了y和x之间的关系），用这两者相减，结果就是拟合了我们的错误指标，用1减去这个商结果就是我们的模型没有产生错误的指标<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-a2bab126bfe18889.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="4.1-3"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-670dd67010cada04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="4.1-4"
                ></p>
<h3 id="4-1-2-实现"><a href="#4-1-2-实现" class="headerlink" title="4.1.2 实现"></a>4.1.2 实现</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">def r2_score(y_true, y_predict):</span><br><span class="line">    &quot;&quot;&quot;计算y_true和y_predict之间的R Square&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    return 1 - mean_squared_error(y_true, y_predict)/np.var(y_true)</span><br></pre></td></tr></table></figure></div>
<p>sikit learn</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.metrics import r2_score</span><br><span class="line">r2_score(y_test,y_predict)</span><br></pre></td></tr></table></figure></div>
<p>将计算分数方法封装到我们的SimpleLinearRegression中</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">from .metrics import r2_score</span><br><span class="line">def score(self, x_test, y_test):</span><br><span class="line">        &quot;&quot;&quot;根据测试数据集 x_test 和 y_test 确定当前模型的准确度&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        y_predict = self.predict(x_test)</span><br><span class="line">        return r2_score(y_test, y_predict)</span><br></pre></td></tr></table></figure></div>



<h1 id="5-多元线性回归"><a href="#5-多元线性回归" class="headerlink" title="5.多元线性回归"></a>5.多元线性回归</h1><h2 id="5-1-多元线性回归简介和正规方程解"><a href="#5-1-多元线性回归简介和正规方程解" class="headerlink" title="5.1 多元线性回归简介和正规方程解"></a>5.1 多元线性回归简介和正规方程解</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-b47942551895e623.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="5.1-1"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-99d5435ff88ae007.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="5.1-2"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-7dd16d69786c892a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="5.1-3"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-bc789f7481a29101.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="5.1-4"
                ></p>
<p>补充（矩阵点乘：A（m行）·B（n列） &#x3D; A的每一行与B的每一列相乘再相加，等到结果是m行n列的）<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-1e6d562cb4d08ac3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="5.1-5"
                ><br>补充（一个1xm的行向量乘以一个mx1的列向量等于一个数）<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-eaded0b058dae228.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="5.1-6"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-4faa4437792aaa7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="5.1-7"
                ><br>推导过程参考 <a class="link"   href="https://blog.csdn.net/nomadlx53/article/details/50849941" >https://blog.csdn.net/nomadlx53/article/details/50849941<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="4-2-多元线性回归实现"><a href="#4-2-多元线性回归实现" class="headerlink" title="4.2 多元线性回归实现"></a>4.2 多元线性回归实现</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-96238af4246f128b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="4.2-1"
                ></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from .metrics import r2_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LinearRegression:</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        &quot;&quot;&quot;初始化Linear Regression模型&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        # 系数向量（θ1,θ2,.....θn）</span><br><span class="line">        self.coef_ = None</span><br><span class="line">        # 截距 (θ0)</span><br><span class="line">        self.interception_ = None</span><br><span class="line">        # θ向量</span><br><span class="line">        self._theta = None</span><br><span class="line"></span><br><span class="line">    def fit_normal(self, X_train, y_train):</span><br><span class="line">        &quot;&quot;&quot;根据训练数据集X_train，y_train 训练Linear Regression模型&quot;&quot;&quot;</span><br><span class="line">        assert X_train.shape[0] == y_train.shape[0], \</span><br><span class="line">            &quot;the size of X_train must be equal to the size of y_train&quot;</span><br><span class="line"></span><br><span class="line">        # np.ones((len(X_train), 1)) 构造一个和X_train 同样行数的，只有一列的全是1的矩阵</span><br><span class="line">        # np.hstack 拼接矩阵</span><br><span class="line">        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])</span><br><span class="line">        # X_b.T 获取矩阵的转置</span><br><span class="line">        # np.linalg.inv() 获取矩阵的逆</span><br><span class="line">        # dot() 矩阵点乘</span><br><span class="line">        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)</span><br><span class="line"></span><br><span class="line">        self.interception_ = self._theta[0]</span><br><span class="line">        self.coef_ = self._theta[1:]</span><br><span class="line"></span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def predict(self, X_predict):</span><br><span class="line">        &quot;&quot;&quot;给定待预测数据集X_predict，返回表示X_predict的结果向量&quot;&quot;&quot;</span><br><span class="line">        assert self.coef_ is not None and self.interception_ is not None,\</span><br><span class="line">            &quot;must fit before predict&quot;</span><br><span class="line">        assert X_predict.shape[1] == len(self.coef_),\</span><br><span class="line">            &quot;the feature number of X_predict must be equal to X_train&quot;</span><br><span class="line"></span><br><span class="line">        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])</span><br><span class="line">        return X_b.dot(self._theta)</span><br><span class="line"></span><br><span class="line">    def score(self, X_test, y_test):</span><br><span class="line">        &quot;&quot;&quot;根据测试数据集 X_test 和 y_test 确定当前模型的准确度&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        y_predict = self.predict(X_test)</span><br><span class="line">        return r2_score(y_test, y_predict)</span><br><span class="line"></span><br><span class="line">    def __repr__(self):</span><br><span class="line">        return &quot;LinearRegression()&quot;</span><br></pre></td></tr></table></figure></div>

<h3 id="预测波士顿房价的测试"><a href="#预测波士顿房价的测试" class="headerlink" title="预测波士顿房价的测试"></a>预测波士顿房价的测试</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plot</span><br><span class="line">from sklearn import datasets</span><br><span class="line"></span><br><span class="line"># 加载波士顿房价数据</span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line">X = X[y&lt;50.0]</span><br><span class="line">y = y[y&lt;50.0]</span><br><span class="line"></span><br><span class="line"># 分割训练集和测试集</span><br><span class="line">from machine_learning.module_selection import train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,seed=666)</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">from machine_learning.LinearRegression import LinearRegression</span><br><span class="line">reg = LinearRegression()</span><br><span class="line">reg.fit_normal(X_train,y_train)</span><br><span class="line"># 输出 LinearRegression()</span><br><span class="line"></span><br><span class="line"># 结果</span><br><span class="line">reg.coef_</span><br><span class="line"># 输出 array([-1.18919477e-01,  3.63991462e-02, -3.56494193e-02,  5.66737830e-02,</span><br><span class="line">       -1.16195486e+01,  3.42022185e+00, -2.31470282e-02, -1.19509560e+00,</span><br><span class="line">        2.59339091e-01, -1.40112724e-02, -8.36521175e-01,  7.92283639e-03,</span><br><span class="line">       -3.81966137e-01])</span><br><span class="line"></span><br><span class="line">reg.interception_</span><br><span class="line"># 输出 34.16143549621706</span><br><span class="line"></span><br><span class="line">reg.score(X_test,y_test)</span><br><span class="line"># 输出 0.812980260265849</span><br></pre></td></tr></table></figure></div>
<h2 id="4-3-scikit-learn中的回归问题"><a href="#4-3-scikit-learn中的回归问题" class="headerlink" title="4.3 scikit-learn中的回归问题"></a>4.3 scikit-learn中的回归问题</h2><h3 id="scikit-learn-中的线性回归"><a href="#scikit-learn-中的线性回归" class="headerlink" title="scikit-learn 中的线性回归"></a>scikit-learn 中的线性回归</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=<span class="number">666</span>)</span><br></pre></td></tr></table></figure></div>


<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">lin_reg = LinearRegression()</span><br></pre></td></tr></table></figure></div>


<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">lin_reg.fit(X_train,y_train)</span><br></pre></td></tr></table></figure></div>




<pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</code></pre>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">lin_reg.coef_</span><br></pre></td></tr></table></figure></div>




<pre><code>array([-1.14235739e-01,  3.12783163e-02, -4.30926281e-02, -9.16425531e-02,
       -1.09940036e+01,  3.49155727e+00, -1.40778005e-02, -1.06270960e+00,
        2.45307516e-01, -1.23179738e-02, -8.80618320e-01,  8.43243544e-03,
       -3.99667727e-01])
</code></pre>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 由于训练数据集和测试数据集的分割和我们的稍有不同，所以结果会略有不同</span></span><br><span class="line">lin_reg.intercept_</span><br></pre></td></tr></table></figure></div>




<pre><code>32.64566083965224
</code></pre>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">lin_reg.score(X_test,y_test)</span><br></pre></td></tr></table></figure></div>




<pre><code>0.8008916199519077
</code></pre>
<h3 id="kNN-Regressor-实现线性回归"><a href="#kNN-Regressor-实现线性回归" class="headerlink" title="kNN Regressor 实现线性回归"></a>kNN Regressor 实现线性回归</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line"></span><br><span class="line">knn_reg = KNeighborsRegressor()</span><br><span class="line">knn_reg.fit(X_train,y_train)</span><br><span class="line">knn_reg.score(X_test,y_test)</span><br></pre></td></tr></table></figure></div>




<pre><code>0.602674505080953
</code></pre>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 网格搜索超参数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">param_grid = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;weights&quot;</span> : [<span class="string">&quot;uniform&quot;</span>],</span><br><span class="line">        <span class="string">&quot;n_neighbors&quot;</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>)]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;weights&quot;</span> : [<span class="string">&quot;distance&quot;</span>],</span><br><span class="line">        <span class="string">&quot;n_neighbors&quot;</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>)],</span><br><span class="line">        <span class="string">&quot;p&quot;</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">6</span>)]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">knn_reg = KNeighborsRegressor()</span><br><span class="line">grid_search = GridSearchCV(knn_reg,param_grid,n_jobs=-<span class="number">1</span>,verbose=<span class="number">1</span>)</span><br><span class="line">grid_search.fit(X_train,y_train)</span><br></pre></td></tr></table></figure></div>

<pre><code>Fitting 3 folds for each of 60 candidates, totalling 180 fits


[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    0.5s finished





GridSearchCV(cv=None, error_score=&#39;raise&#39;,
       estimator=KNeighborsRegressor(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
          metric_params=None, n_jobs=1, n_neighbors=5, p=2,
          weights=&#39;uniform&#39;),
       fit_params=None, iid=True, n_jobs=-1,
       param_grid=[&#123;&#39;weights&#39;: [&#39;uniform&#39;], &#39;n_neighbors&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&#125;, &#123;&#39;weights&#39;: [&#39;distance&#39;], &#39;n_neighbors&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], &#39;p&#39;: [1, 2, 3, 4, 5]&#125;],
       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,
       scoring=None, verbose=1)
</code></pre>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">grid_search.best_params_</span><br></pre></td></tr></table></figure></div>




<pre><code>&#123;&#39;n_neighbors&#39;: 6, &#39;p&#39;: 1, &#39;weights&#39;: &#39;distance&#39;&#125;
</code></pre>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 运用了CV交叉验证的方式</span></span><br><span class="line">grid_search.best_score_</span><br></pre></td></tr></table></figure></div>




<pre><code>0.6060327991735741
</code></pre>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">grid_search.best_estimator_.score(X_test,y_test)</span><br></pre></td></tr></table></figure></div>




<pre><code>0.7354244906092771
</code></pre>
<h1 id="6-线性回归的可解性和更多思考"><a href="#6-线性回归的可解性和更多思考" class="headerlink" title="6.线性回归的可解性和更多思考"></a>6.线性回归的可解性和更多思考</h1><h2 id="6-1可解释性"><a href="#6-1可解释性" class="headerlink" title="6.1可解释性"></a>6.1可解释性</h2><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X,y)</span><br><span class="line">lin_reg.coef_</span><br><span class="line"># 输出：array([-1.05574295e-01,  3.52748549e-02, -4.35179251e-02,  4.55405227e-01,</span><br><span class="line">       -1.24268073e+01,  3.75411229e+00, -2.36116881e-02, -1.21088069e+00,</span><br><span class="line">        2.50740082e-01, -1.37702943e-02, -8.38888137e-01,  7.93577159e-03,</span><br><span class="line">       -3.50952134e-01])</span><br><span class="line"># 将特征结果坐标排序</span><br><span class="line">np.argsort(lin_reg.coef_)</span><br><span class="line"># 输出：array([ 4,  7, 10, 12,  0,  2,  6,  9, 11,  1,  8,  3,  5])</span><br><span class="line"></span><br><span class="line"># 将排序过后的坐标对应的名称展示出来，方便观察理解</span><br><span class="line">boston.feature_names[np.argsort(lin_reg.coef_)]</span><br><span class="line"># 输出：array([&#x27;NOX&#x27;, &#x27;DIS&#x27;, &#x27;PTRATIO&#x27;, &#x27;LSTAT&#x27;, &#x27;CRIM&#x27;, &#x27;INDUS&#x27;, &#x27;AGE&#x27;, &#x27;TAX&#x27;,</span><br><span class="line">       &#x27;B&#x27;, &#x27;ZN&#x27;, &#x27;RAD&#x27;, &#x27;CHAS&#x27;, &#x27;RM&#x27;], dtype=&#x27;&lt;U7&#x27;)</span><br></pre></td></tr></table></figure></div>

<p>RM对应的是房间数，是正相关最大的特征，也就是说房间数越多，房价越高，这是很合理的<br>NOX对应的是一氧化氮浓度，也就是说一氧化氮浓度越低，房价越低，这也是非常合理的<br>由此说明，我们的线性回归具有可解释性，我们可以在对研究一个模型的时候，可以先用线性回归模型看一下，然后根据感性的认识去直观的判断一下是否符合我们的语气</p>
<h2 id="6-2-总结"><a href="#6-2-总结" class="headerlink" title="6.2 总结"></a>6.2 总结</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-207d239d640630b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="6.2-1"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-e3a2b28f8657e2b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="6.2-2"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-57f8e00af3e5987f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="6.2-3"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://upload-images.jianshu.io/upload_images/7220971-3ae395d5fb12edd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"
                      alt="6.2-4"
                ></p>
]]></content>
  </entry>
</search>
